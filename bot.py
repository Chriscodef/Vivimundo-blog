#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import time
import json
import requests
from datetime import datetime
from pathlib import Path
import subprocess
import random
from bs4 import BeautifulSoup
import urllib3

# Desabilitar SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

sys.stdout.reconfigure(line_buffering=True, encoding='utf-8')
sys.stderr.reconfigure(line_buffering=True, encoding='utf-8')
os.environ['PYTHONIOENCODING'] = 'utf-8'

def log(msg):
    print(msg, flush=True)

# Configura√ß√µes
GROQ_API_KEY = os.getenv('GROQ_API_KEY')
GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')
REPO_PATH = os.getenv('GITHUB_WORKSPACE', '.')
if not GROQ_API_KEY:
    log("‚ùå GROQ_API_KEY n√£o encontrada!")
    sys.exit(1)

# Arquivo para salvar estado
STATE_FILE = Path(REPO_PATH) / "bot_state.json"
ARTICLES_CACHE = Path(REPO_PATH) / "articles_cache.json"

def carregar_cache_artigos():
    """Carrega URLs e t√≠tulos j√° processados"""
    if ARTICLES_CACHE.exists():
        with open(ARTICLES_CACHE, 'r') as f:
            data = json.load(f)
            if isinstance(data, dict):
                return set(data.get('urls', [])), set(data.get('titulos', []))
            # Compatibilidade com formato antigo (apenas URLs)
            return set(data), set()
    return set(), set()

def salvar_cache_artigos(urls, titulos):
    """Salva URLs e t√≠tulos processados"""
    with open(ARTICLES_CACHE, 'w') as f:
        json.dump({'urls': list(urls), 'titulos': list(titulos)}, f)

def normalizar_url(url):
    """Normaliza URL para compara√ß√£o consistente no cache"""
    from urllib.parse import urlparse, urlunparse, parse_qs, urlencode
    
    # Converte para lowercase
    url = url.lower().strip()
    
    # Remove trailing slash
    if url.endswith('/'):
        url = url[:-1]
    
    # Parse URL
    parsed = urlparse(url)
    
    # Remove par√¢metros de tracking comuns (utm_, fbclid, etc)
    query_params = parse_qs(parsed.query)
    params_limpos = {k: v for k, v in query_params.items() 
                     if not k.startswith(('utm_', 'fbclid', 'gclid', 'ref'))}
    
    # Reconstr√≥i query string ordenada
    nova_query = urlencode(params_limpos, doseq=True) if params_limpos else ''
    
    # Reconstr√≥i URL normalizada
    return urlunparse((
        parsed.scheme,
        parsed.netloc,
        parsed.path,
        parsed.params,
        nova_query,
        ''  # Remove fragmento (#)
    ))

def normalizar_titulo(titulo):
    """Normaliza t√≠tulo para detec√ß√£o de duplicatas"""
    import re
    # Remove espa√ßos extras, converte para lowercase
    titulo = titulo.lower().strip()
    # Remove pontua√ß√£o
    titulo = re.sub(r'[^\w\s]', '', titulo)
    # Remove espa√ßos m√∫ltiplos
    titulo = re.sub(r'\s+', ' ', titulo)
    return titulo

def titulo_similar(titulo_novo, titulos_existentes, limiar=0.65):
    """Verifica se um t√≠tulo √© similar a algum j√° existente usando compara√ß√£o de palavras.
    Retorna True se encontrar um t√≠tulo com similaridade >= limiar (0.65 = 65%)."""
    palavras_novo = set(normalizar_titulo(titulo_novo).split())
    if len(palavras_novo) < 3:
        return False
    
    for titulo_existente in titulos_existentes:
        palavras_existente = set(titulo_existente.split())
        if len(palavras_existente) < 3:
            continue
        
        # Calcula similaridade de Jaccard (interse√ß√£o / uni√£o)
        intersecao = palavras_novo & palavras_existente
        uniao = palavras_novo | palavras_existente
        similaridade = len(intersecao) / len(uniao) if uniao else 0
        
        if similaridade >= limiar:
            log(f"  üîÑ T√≠tulo similar ({similaridade:.0%}): {titulo_novo[:50]}...")
            return True
    
    return False

def eh_imagem_valida(img_url):
    """Verifica se a URL da imagem √© real (n√£o √© placeholder, logo, etc)"""
    if not img_url:
        return False
    
    img_lower = img_url.lower()
    
    # Rejeita placeholders conhecidos
    placeholders_bloqueados = [
        'via.placeholder.com',
        'placeholder.com',
        'placehold.it',
        'placekitten.com',
        'picsum.photos',
        'dummyimage.com',
        'fakeimg.pl',
        'lorempixel.com',
        'loremflickr.com',
        'placeholderimage',
        'default-image',
        'no-image',
        'noimage',
        'sem-imagem',
        'image-not-found',
        'img-placeholder',
    ]
    
    if any(placeholder in img_lower for placeholder in placeholders_bloqueados):
        log(f"  üö´ Imagem placeholder rejeitada: {img_url[:60]}...")
        return False
    
    # Rejeita imagens muito pequenas (√≠cones, badges)
    extensoes_invalidas = ['.ico', '.svg', '.gif']
    if any(img_lower.endswith(ext) for ext in extensoes_invalidas):
        # SVGs e GIFs podem ser v√°lidos se forem grandes, mas geralmente s√£o logos
        if 'logo' in img_lower or 'icon' in img_lower or 'badge' in img_lower:
            log(f"  üö´ Imagem logo/√≠cone rejeitada: {img_url[:60]}...")
            return False
    
    # Rejeita data URIs (base64 inline images geralmente s√£o √≠cones)
    if img_lower.startswith('data:'):
        return False
    
    # Rejeita URLs que s√£o claramente logos ou avatares
    palavras_logo = ['logo', 'favicon', 'avatar', 'profile-pic', 'user-icon', 'brand']
    if any(palavra in img_lower for palavra in palavras_logo):
        log(f"  üö´ Imagem logo/avatar rejeitada: {img_url[:60]}...")
        return False
    
    return True


def carregar_estado():
    """Carrega o √≠ndice do √∫ltimo tema executado"""
    if STATE_FILE.exists():
        with open(STATE_FILE, 'r') as f:
            state = json.load(f)
            return state.get('tema_idx', 0), state.get('total_posts', 0)
    return 0, 0

def salvar_estado(tema_idx, total_posts):
    """Salva o √≠ndice do tema para pr√≥xima execu√ß√£o"""
    with open(STATE_FILE, 'w') as f:
        json.dump({'tema_idx': tema_idx, 'total_posts': total_posts}, f)

TEMAS = [
    {"nome": "Esportes", "categoria": "esportes", "sites": [
        "https://ge.globo.com/", "https://www.espn.com.br/", "https://www.uol.com.br/esporte/",
        "https://www.espn.com.br/futebol/", "https://www.grandepremio.com.br/",
        "https://www.lance.com.br/", "https://www.gazetaesportiva.com/",
    ]},
    {"nome": "Entretenimento", "categoria": "entretenimento", "sites": [
        "https://www.omelete.com.br/", "https://www.tecmundo.com.br/cultura",
        "https://noticiasdocinema.com.br/", "https://www.adorocinema.com/",
        "https://www.papelpop.com/", "https://rollingstone.com.br/",
    ]},
    {"nome": "Tecnologia", "categoria": "tecnologia", "sites": [
        "https://www.tecmundo.com.br/", "https://olhardigital.com.br/",
        "https://www.hardware.com.br/", "https://tecnoblog.net/",
        "https://canaltech.com.br/", "https://www.tudocelular.com/",
    ]},
    {"nome": "Videogames", "categoria": "videogames", "sites": [
        "https://www.gamerant.com/", "https://br.ign.com/",
        "https://www.thegamer.com.br/", "https://www.tecmundo.com.br/voxel",
        "https://www.theenemy.com.br/",
    ]},
    {"nome": "Pol√≠tica Nacional", "categoria": "politica-nacional", "sites": [
        "https://g1.globo.com/politica/", "https://noticias.uol.com.br/politica/",
        "https://www.poder360.com.br/", "https://www.cnnbrasil.com.br/politica/",
        "https://www.cartacapital.com.br/politica/",
    ]},
    {"nome": "Pol√≠tica Internacional", "categoria": "politica-internacional", "sites": [
        "https://g1.globo.com/mundo/", "https://www.bbc.com/portuguese/internacional",
        "https://noticias.uol.com.br/internacional/", "https://hojenomundomilitar.com.br/",
        "https://www.cnnbrasil.com.br/internacional/",
    ]},
    {"nome": "Rio de Janeiro", "categoria": "rio-de-janeiro", "sites": [
        "https://g1.globo.com/rj/rio-de-janeiro/", "https://odia.ig.com.br/",
        "https://diariodorio.com/", "https://www.band.uol.com.br/band-news-fm/rio",
        "https://extra.globo.com/noticias/rio/",
    ]},
    {"nome": "S√£o Paulo", "categoria": "sao-paulo", "sites": [
        "https://g1.globo.com/sp/sao-paulo/", "https://www.band.uol.com.br/band-news-fm/sp",
        "https://noticias.r7.com/sao-paulo/", "https://agora.folha.uol.com.br/sao-paulo/",
    ]},
]


HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
}

def setup_repo():
    try:
        log("üìÇ Configurando Git...")
        subprocess.run(['git', 'config', 'user.name', 'Vivimundo Bot'], check=True)
        subprocess.run(['git', 'config', 'user.email', 'bot@vivimundo.com'], check=True)
        if GITHUB_TOKEN:
            repo_url = f'https://{GITHUB_TOKEN}@github.com/Chriscodef/Vivimundo-blog.git'
            subprocess.run(['git', 'remote', 'remove', 'origin'], capture_output=True)
            subprocess.run(['git', 'remote', 'add', 'origin', repo_url], check=True, capture_output=True)
        subprocess.run(['git', 'pull', 'origin', 'main', '--rebase'], check=False)
        log("‚úÖ Git OK")
        return True
    except Exception as e:
        log(f"‚ö†Ô∏è {e}")
        return True

def extrair_imagem_meta(soup, url):
    """Extrai imagem de meta tags (og:image, twitter:image)"""
    try:
        # Tenta og:image primeiro
        img = soup.find('meta', property='og:image')
        if img and img.get('content'):
            return img['content']
        
        # Tenta twitter:image
        img = soup.find('meta', attrs={'name': 'twitter:image'})
        if img and img.get('content'):
            return img['content']
        
        # Tenta img com classe espec√≠fica
        img = soup.find('img', class_=lambda x: x and any(palavra in str(x).lower() for palavra in ['article', 'post', 'destaque', 'noticia', 'manchete']))
        if img and img.get('src'):
            return img['src']
    except:
        pass
    return None

def limpar_titulo(titulo):
    """Limpa t√≠tulos com palavras grudadas (ex: 'JacksonVeja' -> 'Jackson Veja')"""
    import re
    
    # Padr√£o 1: letra min√∫scula seguida de mai√∫scula (ex: "jacksonVeja")
    titulo = re.sub(r'([a-z√†-√∫])([A-Z√Ä-√ö])', r'\1 \2', titulo)
    
    # Padr√£o 2: pontua√ß√£o seguida de letra sem espa√ßo (ex: "A√ç!Baldur's", "ok.Veja")
    titulo = re.sub(r'([!?:.\)\]])([A-Z√Ä-√öa-z√†-√∫])', r'\1 \2', titulo)
    
    # Padr√£o 3: palavra completamente mai√∫scula seguida de palavra capitalizada (ex: "HPComo")
    titulo = re.sub(r'([A-Z√Ä-√ö]{2,})([A-Z√Ä-√ö][a-z√†-√∫])', r'\1 \2', titulo)
    
    # Padr√£o 4: d√≠gito seguido de letra mai√∫scula sem espa√ßo (ex: "9Ganha")
    titulo = re.sub(r'(\d)([A-Z√Ä-√ö])', r'\1 \2', titulo)
    
    # Padr√£o 5: letra seguida de d√≠gito colado em contexto estranho (ex: "veja3motivos")
    titulo = re.sub(r'([a-z√†-√∫])(\d+)([A-Z√Ä-√ö])', r'\1 \2 \3', titulo)
    
    # Padr√£o 6: fecha aspas/par√™nteses colado em pr√≥xima palavra
    titulo = re.sub(r'(["\'¬ª])([A-Z√Ä-√öa-z√†-√∫])', r'\1 \2', titulo)
    
    # Remove espa√ßos m√∫ltiplos
    titulo = re.sub(r'\s+', ' ', titulo)
    
    return titulo.strip()

def eh_titulo_valido(titulo):
    """Valida se o t√≠tulo √© real (n√£o √© n√∫mero de telefone, sequ√™ncia, etc)"""
    import re
    # Remove espa√ßos extras
    titulo = titulo.strip()
    
    # Muito curto ou longo
    if len(titulo) < 20 or len(titulo) > 250:
        log(f"  üö´ T√≠tulo rejeitado (tamanho {len(titulo)}): {titulo[:60]}...")
        return False
    
    # Parece n√∫mero de telefone ou ID
    if titulo.replace('-', '').replace('(', '').replace(')', '').isdigit():
        return False
    
    # Muitos n√∫meros (telefone, CEP, etc)
    num_count = sum(1 for c in titulo if c.isdigit())
    if num_count > len(titulo) * 0.3:  # Mais de 30% n√∫meros
        return False
    
    # Palavras v√°lidas m√≠nimas (n√£o √© s√≥ n√∫meros e s√≠mbolos)
    palavras = [p for p in titulo.split() if len(p) > 2 and not p.isdigit()]
    if len(palavras) < 3:  # Menos de 3 palavras v√°lidas
        log(f"  üö´ T√≠tulo rejeitado (poucas palavras): {titulo[:60]}...")
        return False
    
    # Rejeita t√≠tulos gen√©ricos de se√ß√£o (n√£o s√£o not√≠cias reais)
    titulo_lower = titulo.lower()
    palavras_secao = [
        'advance', 'latest', 'more', 'daily', 'special', 'featured',
        'esportes a motor', 'game rant', 'puzzles and games',
        'trending', 'popular', 'recommended', 'breaking',
        'read more', 'see more', 'leia mais', 'veja mais', 'saiba mais',
        'menu principal', 'navega√ß√£o', 'buscar', 'pesquisar',
        'home', 'in√≠cio', 'voltar', 'anterior', 'pr√≥ximo',
        'cookies', 'privacidade', 'termos de uso',
        'sign in', 'sign up', 'subscribe', 'follow us',
        'all rights reserved', 'todos os direitos',
        'not√≠cias recentes', 'mais lidas', 'mais populares',
        'editor picks', 'top stories', 'highlights',
        'the gamer', 'ign brasil', 'tecmundo', 'olhar digital',
        'game reviews', 'movie reviews', 'tv reviews',
        'about us', 'contact us', 'advertise',
    ]
    for palavra in palavras_secao:
        if titulo_lower == palavra or titulo_lower.startswith(palavra + ' ') or titulo_lower.endswith(' ' + palavra):
            log(f"  üö´ T√≠tulo rejeitado (se√ß√£o gen√©rica): {titulo[:60]}...")
            return False
        # Rejeita se o t√≠tulo inteiro √© basicamente a palavra de se√ß√£o
        if palavra in titulo_lower and len(titulo) < len(palavra) + 15:
            log(f"  üö´ T√≠tulo rejeitado (se√ß√£o gen√©rica curta): {titulo[:60]}...")
            return False
    
    # Rejeita t√≠tulos que s√£o apenas nomes de categorias/se√ß√µes do site
    titulos_exatos_bloqueados = [
        'esportes', 'entretenimento', 'tecnologia', 'videogames', 'games',
        'pol√≠tica', 'economia', 'mundo', 'brasil', 'cultura', 'ci√™ncia',
        'sa√∫de', 'educa√ß√£o', 'opini√£o', 'editorial', 'colunistas',
        'esportes a motor', 'automobilismo', 'futebol', 'basquete',
        'game rant advance', 'ign recommends', 'editor choice',
    ]
    if titulo_lower in titulos_exatos_bloqueados:
        log(f"  üö´ T√≠tulo rejeitado (nome de categoria): {titulo[:60]}...")
        return False
    
    # Rejeita t√≠tulos muito curtos com poucas palavras significativas (provavelmente categorias)
    palavras_significativas = [p for p in titulo.split() if len(p) > 3 and p.isalpha()]
    if len(palavras_significativas) < 4:
        # Verifica se parece uma categoria (sem verbos de a√ß√£o)
        verbos_acao = ['ganha', 'lan√ßa', 'confirma', 'aprova', 'revela', 'anuncia',
                       'chega', 'vence', 'perde', 'encontra', 'descobre', 'morre',
                       'nasce', 'cresce', 'cai', 'sobe', 'muda', 'fica', 'vai', 'vem',
                       'diz', 'afirma', 'declara', 'promete', 'nega', 'acusa',
                       'mostra', 'apresenta', 'estreia', 'lan√ßa', 'recebe',
                       'wins', 'loses', 'announces', 'reveals', 'launches', 'gets',
                       'shows', 'confirms', 'releases', 'updates', 'adds',
                       'pode', 'deve', 'ser√°', 'est√°', 'foi', 'tem', 'faz',
                       'volta', 'entra', 'sai', 'abre', 'fecha', 'inicia',
                       'atinge', 'supera', 'bate', 'quebra', 'alcan√ßa']
        tem_verbo = any(verbo in titulo_lower for verbo in verbos_acao)
        if not tem_verbo:
            log(f"  üö´ T√≠tulo rejeitado (sem verbo de a√ß√£o): {titulo[:60]}...")
            return False
    
    # Rejeita t√≠tulos que parecem ser menus ou listas de navega√ß√£o
    if titulo.count('|') > 1 or titulo.count('‚Ä∫') > 1 or titulo.count('¬ª') > 1:
        log(f"  üö´ T√≠tulo rejeitado (parece navega√ß√£o): {titulo[:60]}...")
        return False
    
    # Rejeita t√≠tulos com muitas palavras em ingl√™s em sites BR (provavelmente UI)
    palavras_en = ['the', 'and', 'for', 'with', 'from', 'this', 'that', 'your', 'our', 'their']
    contagem_en = sum(1 for p in titulo_lower.split() if p in palavras_en)
    if contagem_en >= 3 and len(titulo.split()) < 8:
        log(f"  üö´ T√≠tulo rejeitado (parece UI em ingl√™s): {titulo[:60]}...")
        return False
    
    return True


def buscar_noticia(tema):
    time.sleep(random.uniform(1, 3))
    urls_processadas, titulos_processados = carregar_cache_artigos()
    
    for site_url in tema['sites']:

        try:
            log(f"  üîç Tentando {site_url}...")
            
            resp = requests.get(site_url, headers=HEADERS, timeout=20, verify=False)
            resp.encoding = 'utf-8'
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # Busca links em artigos, posts ou se√ß√µes de not√≠cias
            links = soup.find_all('a', href=True)
            links = links[:80]  # Aumentar para buscar mais links
            
            for link in links:
                href = link.get('href', '')
                titulo = link.get_text(strip=True)
                
                # Limpa t√≠tulos grudados
                titulo = limpar_titulo(titulo)
                
                # Valida t√≠tulo
                if not eh_titulo_valido(titulo):

                    continue
                
                # Palavras-chave para excluir
                palavras_bloqueadas = [
                    'publicidade', 'an√∫ncio', 'assine', 'login', 'cadastro', 'newsletter',
                    'amazon', 'aliexpress', 'mercado livre', 'shopee', 'custo', 'pre√ßo',
                    'compre', 'oferta', 'desconto', 'cupom', 'promo√ß√£o', 'black friday',
                    'aviso', 'clique', 'compartilhe', 'siga', 'inscreva', 'download',
                    'vpn', 'antiv√≠rus', 'norton', 'testegr√°tis', 'teste gr√°tis', '% off', '% offert',
                    'c√≥digo', 'cupom', 'deal', 'cyber', 'viagem', 'hotel', 'passagem',
                    'fone', 'fones', 'headphone', 'smartphone', 'iphone', 'samsung'
                ]
                
                if any(palavra in titulo.lower() for palavra in palavras_bloqueadas):
                    continue
                
                # Formata URL relativa
                if href.startswith('/'):
                    from urllib.parse import urljoin
                    href = urljoin(site_url, href)
                
                if not href.startswith('http'):
                    continue
                
                # Normaliza URL para verifica√ß√£o
                href_normalizada = normalizar_url(href)
                
                # Pula URL j√° processada (verifica√ß√£o normalizada)
                if href_normalizada in urls_processadas:
                    log(f"  üîÑ URL j√° processada: {href[:50]}...")
                    continue
                
                # Verifica duplicata por t√≠tulo normalizado (exato)
                titulo_normalizado = normalizar_titulo(titulo)
                if titulo_normalizado in titulos_processados:
                    log(f"  üîÑ T√≠tulo duplicado (exato): {titulo[:50]}...")
                    continue
                
                # Verifica duplicata por similaridade (fuzzy matching)
                if titulo_similar(titulo, titulos_processados):
                    continue
                
                # Bloqueia links para plataformas de compra
                urls_bloqueadas = ['amazon.com', 'aliexpress.com', 'mercadolivre.com', 'shopee.com', 'ebay.com']
                if any(bloqueado in href.lower() for bloqueado in urls_bloqueadas):
                    continue
                
                try:
                    time.sleep(random.uniform(0.7, 1.5))
                    
                    # Acessa artigo
                    art_resp = requests.get(href, headers=HEADERS, timeout=20, verify=False)
                    art_resp.encoding = 'utf-8'
                    art_soup = BeautifulSoup(art_resp.text, 'html.parser')
                    
                    # Remove lixo
                    for tag in art_soup(['script', 'style', 'nav', 'footer', 'aside']):
                        tag.decompose()
                    
                    # Busca conte√∫do em par√°grafos
                    # OBS: usar separator=" " evita palavras grudadas quando h√° tags inline (<a>, <strong>, etc.)
                    paragrafos = art_soup.find_all('p')
                    texto = ' '.join(
                        p.get_text(" ", strip=True)
                        for p in paragrafos
                        if len(p.get_text(" ", strip=True)) > 30
                    )
                    
                    # Se n√£o encontrou em <p>, tenta em divs com classes de artigo
                    if len(texto) < 400:
                        article = art_soup.find(['article', 'div', 'main'], class_=lambda x: x and any(palavra in str(x).lower() for palavra in ['article', 'post', 'content', 'corpo', 'noticia', 'body', 'text']))
                        if article:
                            paragrafos = article.find_all('p')
                            texto = ' '.join(
                                p.get_text(" ", strip=True)
                                for p in paragrafos
                                if len(p.get_text(" ", strip=True)) > 30
                            )
                    
                    # Busca imagem com fun√ß√£o melhorada
                    img_url = extrair_imagem_melhorada(art_soup, href)
                    
                    # Formata URL da imagem
                    if img_url and not img_url.startswith('http'):
                        from urllib.parse import urljoin
                        img_url = urljoin(href, img_url)
                    
                    # Rejeita not√≠cias sem imagem real ou com placeholder
                    if not eh_imagem_valida(img_url):
                        log(f"  üö´ Not√≠cia sem imagem v√°lida, pulando: {titulo[:50]}...")
                        urls_processadas.add(href_normalizada)
                        titulos_processados.add(titulo_normalizado)
                        salvar_cache_artigos(urls_processadas, titulos_processados)
                        continue
                    
                    # Valida conte√∫do
                    if len(texto) > 500:
                        log(f"  ‚úÖ Encontrada: {titulo[:60]}...")
                        # Marca como processada (URL normalizada e t√≠tulo)
                        urls_processadas.add(href_normalizada)
                        titulos_processados.add(titulo_normalizado)
                        salvar_cache_artigos(urls_processadas, titulos_processados)
                        return {
                            'title': titulo, 
                            'content': texto, 
                            'urlToImage': img_url, 
                            'url': href
                        }

                    else:
                        # Marca como processada mesmo sem conte√∫do suficiente
                        urls_processadas.add(href_normalizada)
                        salvar_cache_artigos(urls_processadas, titulos_processados)


                except requests.exceptions.Timeout:
                    log(f"  ‚è± Timeout em {href[:40]}")
                    continue
                except Exception as e:
                    continue
            
            log(f"  ‚ö†Ô∏è Nada encontrado em {site_url}")
        except Exception as e:
            log(f"  ‚ùå Erro em {site_url}: {str(e)[:60]}")
            continue
    
    return None

def limpar_markdown(texto):
    """Remove formata√ß√£o markdown do texto"""
    import re
    # Remove **texto** -> texto
    texto = re.sub(r'\*\*(.*?)\*\*', r'\1', texto)
    # Remove *texto* -> texto
    texto = re.sub(r'\*(.*?)\*', r'\1', texto)
    # Remove __texto__ -> texto
    texto = re.sub(r'__(.*?)__', r'\1', texto)
    # Remove # titulo -> titulo
    texto = re.sub(r'^#+\s+', '', texto, flags=re.MULTILINE)
    # Remove tags HTML malformadas
    texto = re.sub(r'<p><h\d>(.*?)</h\d></p>', r'\1', texto)
    texto = re.sub(r'<p><p>(.*?)</p></p>', r'\1', texto)
    # Remove tags HTML abertas
    texto = re.sub(r'<h\d>|</h\d>', '', texto)
    return texto

def formatar_paragrafos(texto):
    """Formata texto em par√°grafos HTML bem estruturados"""
    import re
    # Limpa markdown primeiro
    texto = limpar_markdown(texto)
    
    # Remove tags HTML restantes
    texto = re.sub(r'<[^>]+>', '', texto)
    
    # Divide em par√°grafos por quebras duplas ou por pontos finais
    blocos = texto.split('\n\n')
    
    html = ""
    for bloco in blocos:
        bloco = bloco.strip()
        if len(bloco) > 50:  # Ignora blocos muito pequenos
            # Remove espa√ßos m√∫ltiplos
            bloco = re.sub(r'\s+', ' ', bloco)
            html += f'<p>{bloco}</p>\n'
    
    return html

def extrair_imagem_melhorada(soup, url):
    """Extrai a melhor imagem do artigo"""
    try:
        # Tenta og:image primeiro (mais confi√°vel)
        img = soup.find('meta', property='og:image')
        if img and img.get('content'):
            img_url = img['content']
            # Evita logos e √≠cones
            if not any(x in img_url.lower() for x in ['logo', 'icon', 'badge', 'avatar', 'profile']):
                return img_url
        
        # Tenta twitter:image
        img = soup.find('meta', attrs={'name': 'twitter:image'})
        if img and img.get('content'):
            return img['content']
        
        # Procura por imagem grande no artigo
        imgs = soup.find_all('img')
        melhor_img = None
        melhor_tamanho = 0
        
        for img in imgs:
            src = img.get('src', '')
            alt = img.get('alt', '')
            
            # Ignora logos, √≠cones, banners pequenos
            if any(x in src.lower() or x in alt.lower() for x in ['logo', 'icon', 'badge', 'avatar', 'gif', 'svg', 'button']):
                continue
            
            # Prefere imagens com atributos de tamanho
            width = img.get('width', '0')
            height = img.get('height', '0')
            try:
                tamanho = int(width) * int(height) if width and height else 0
                if tamanho > melhor_tamanho:
                    melhor_tamanho = tamanho
                    melhor_img = src
            except:
                if src and not melhor_img:
                    melhor_img = src
        
        return melhor_img
    except:
        pass
    return None

def gerar_texto_fallback(noticia):
    """Gera texto com fallback quando Groq falha"""
    titulo = noticia['title']
    conteudo = noticia.get('content', '')[:2000]

    # Se o conte√∫do extra√≠do n√£o parece PT-BR, melhor abortar do que publicar texto ruim.
    # (Isso evita posts "s√≥ scraped" em ingl√™s ou com lixo.)
    if not parece_portugues(conteudo):
        log("  üö´ Fallback abortado: conte√∫do extra√≠do n√£o parece PT-BR")
        return None
    
    # Estrutura b√°sica de mat√©ria
    paragrafos = conteudo.split('\n\n')
    texto = f"{titulo}\n\n"
    
    for i, p in enumerate(paragrafos[:10]):
        if len(p.strip()) > 50:
            texto += f"{p.strip()}\n\n"
    
    # Se ficou muito curto, repete o conte√∫do
    if len(texto) < 800:
        texto += "\n" + conteudo
    
    return texto[:3000]  # Limita a 3000 caracteres


def parece_portugues(texto: str) -> bool:
    """Heur√≠stica simples para detectar se o texto parece PT-BR.
    N√£o √© um detector perfeito; √© s√≥ para bloquear casos √≥bvios de ingl√™s/UI."""
    if not texto:
        return False

    t = texto.lower()
    # remove tags
    import re
    t = re.sub(r'<[^>]+>', ' ', t)
    t = re.sub(r'\s+', ' ', t).strip()
    if len(t) < 200:
        return False

    tokens = re.findall(r"[a-z√†-√∫]+", t)
    if len(tokens) < 40:
        return False

    pt_stop = {
        'que', 'de', 'do', 'da', 'em', 'para', 'com', 'n√£o', 'uma', 'um', 'os', 'as',
        'por', 'mais', 'como', 'sobre', 'ap√≥s', 'antes', 'entre', 'tamb√©m', 'j√°',
        'foi', 'ser√°', 's√£o', 'era', 'est√°', 'est√£o', 'disse', 'diz', 'ainda',
        'ao', 'aos', '√†', '√†s', 'no', 'na', 'nos', 'nas', 'se', 'sua', 'seu'
    }
    en_stop = {
        'the', 'and', 'for', 'with', 'from', 'this', 'that', 'your', 'our', 'their',
        'you', 'they', 'we', 'was', 'were', 'are', 'is', 'in', 'on', 'of', 'to'
    }

    pt_hits = sum(1 for tok in tokens if tok in pt_stop)
    en_hits = sum(1 for tok in tokens if tok in en_stop)

    # presen√ßa de acentos ajuda
    acentos = sum(1 for ch in t if ch in '√°√†√¢√£√©√™√≠√≥√¥√µ√∫√ß')

    # decis√µes simples
    if en_hits > pt_hits * 2 and en_hits > 20:
        return False
    if pt_hits >= 8:
        return True
    if acentos >= 8:
        return True
    return False


def corrigir_espacamento(texto: str) -> str:
    """Corre√ß√µes leves de espa√ßamento/pontua√ß√£o para reduzir 'palavras grudadas'."""
    import re
    if not texto:
        return texto
    # espa√ßos ap√≥s pontua√ß√£o
    texto = re.sub(r'([,;:.!?])(\S)', r'\1 \2', texto)
    # min√∫scula+Mai√∫scula coladas
    texto = re.sub(r'([a-z√†-√∫])([A-Z√Ä-√ö])', r'\1 \2', texto)
    # d√≠gito+letra colados
    texto = re.sub(r'(\d)([A-Za-z√Ä-√ö√†-√∫])', r'\1 \2', texto)
    # remove espa√ßos m√∫ltiplos
    texto = re.sub(r'\s+', ' ', texto)
    return texto.strip()


def remover_mencoes_de_fonte(texto: str) -> tuple[str, bool]:
    """Remove/neutraliza men√ß√µes a ve√≠culos/fontes.
    Retorna (texto_limpo, houve_remocao)."""
    import re
    if not texto:
        return texto, False

    original = texto
    t = texto

    # remove padr√µes do tipo "Fonte: ..." em qualquer lugar
    t = re.sub(r'(?im)^\s*fonte\s*:\s*.*$', '', t)
    t = re.sub(r'(?im)^\s*source\s*:\s*.*$', '', t)

    # neutraliza men√ß√µes expl√≠citas a portais comuns
    veiculos = [
        'g1', 'uol', 'folha', 'folhapress', 'poder360', 'cnn brasil', 'bbc',
        'ge.globo', 'globo', 'oglobo', 'estad√£o', 'estadao', 'r7', 'ig',
        'omelete', 'tecmundo', 'olhar digital', 'tecnoblog', 'canaltech',
        'ign', 'game rant', 'thegamer', 'the enemy'
    ]
    for v in veiculos:
        # remove "segundo <ve√≠culo>", "de acordo com <ve√≠culo>", "conforme <ve√≠culo>"
        t = re.sub(rf'(?i)(segundo|de acordo com|conforme|reportou|informou)\s+{re.escape(v)}\b', r'\1 informa√ß√µes dispon√≠veis', t)
        t = re.sub(rf'(?i)\b{re.escape(v)}\b', v)  # mant√©m a palavra se estiver no meio, mas reduz chance de apagar sentido

    # remove sobras de linhas vazias
    t = re.sub(r'\n{3,}', '\n\n', t).strip()

    return t, (t != original)


def remover_primeiro_paragrafo_se_repetir_titulo(texto: str, titulo: str) -> tuple[str, bool]:
    """Se o 1¬∫ par√°grafo for basicamente o t√≠tulo (ou come√ßar repetindo), remove."""
    import re
    if not texto or not titulo:
        return texto, False

    # quebra por par√°grafos (linhas em branco)
    partes = [p.strip() for p in re.split(r'\n\s*\n', texto) if p.strip()]
    if len(partes) < 2:
        return texto, False

    t_norm = normalizar_titulo(titulo)
    p0_norm = normalizar_titulo(partes[0])

    # se o primeiro par√°grafo cont√©m o t√≠tulo (ou grande parte dele)
    if t_norm and (t_norm in p0_norm or p0_norm.startswith(t_norm[: max(20, len(t_norm) // 2)])):
        partes = partes[1:]
        return '\n\n'.join(partes).strip(), True

    # Jaccard simples com palavras
    palavras_t = set(t_norm.split())
    palavras_p0 = set(p0_norm.split())
    if palavras_t and palavras_p0:
        sim = len(palavras_t & palavras_p0) / len(palavras_t | palavras_p0)
        if sim >= 0.70:
            partes = partes[1:]
            return '\n\n'.join(partes).strip(), True

    return texto, False


def avaliar_qualidade_materia(titulo: str, texto: str) -> list[str]:
    """Retorna uma lista de flags com problemas detectados."""
    flags = []
    if not texto or len(texto) < 800:
        flags.append('curto')
    if not parece_portugues(texto):
        flags.append('nao_ptbr')

    # sinais de fonte
    tl = texto.lower()
    if 'fonte:' in tl or 'source:' in tl or 'segundo ' in tl or 'de acordo com ' in tl or 'conforme ' in tl:
        flags.append('menciona_fonte')

    # markdown (o prompt pede HTML simples)
    if any(x in texto for x in ['**', '__', '```']):
        flags.append('markdown')

    # t√≠tulo repetido no come√ßo
    t_norm = normalizar_titulo(titulo) if titulo else ''
    inicio = normalizar_titulo(texto[:400])
    if t_norm and t_norm in inicio:
        flags.append('repete_titulo')

    return flags

def gerar_texto(noticia):
    prompt = f"""Escreva uma mat√©ria jornal√≠stica completa em portugu√™s brasileiro (m√≠nimo 450 palavras, par√°grafos, tom profissional) sobre:

T√≠tulo: {noticia['title']}
Conte√∫do: {noticia.get('content', '')[:3000]}

Regras obrigat√≥rias:
- N√ÉO mencione nem cite ve√≠culos, jornais, sites, autores ou links.
- N√ÉO repita o t√≠tulo como primeiro par√°grafo.
- Escreva somente em portugu√™s brasileiro.
- Use apenas HTML simples (sem markdown)."""

    prompt_strito = f"""Reescreva e melhore a mat√©ria abaixo em portugu√™s brasileiro.

T√≠tulo: {noticia['title']}
Conte√∫do base: {noticia.get('content', '')[:3000]}

Regras obrigat√≥rias (n√£o quebre):
1) Texto 100% PT-BR (sem frases em ingl√™s).
2) N√ÉO mencionar fontes/ve√≠culos (G1, UOL, Folha, BBC, etc.) nem express√µes tipo "segundo o jornal".
3) N√ÉO repetir o t√≠tulo no primeiro par√°grafo.
4) Corrigir palavras coladas e erros de espa√ßamento/pontua√ß√£o.
5) Produzir par√°grafos e usar somente HTML simples (<p>, <strong>, <em>) sem markdown.
"""
    try:
        def chamar_groq(conteudo_prompt: str, temperature: float, timeout: int):
            r = requests.post(
                'https://api.groq.com/openai/v1/chat/completions',
                headers={'Authorization': f'Bearer {GROQ_API_KEY}', 'Content-Type': 'application/json'},
                json={
                    'model': 'llama-3.3-70b-versatile',
                    'messages': [{'role': 'user', 'content': conteudo_prompt}],
                    'temperature': temperature,
                    'max_tokens': 2000
                },
                timeout=timeout
            )
            r.raise_for_status()
            return r.json()['choices'][0]['message']['content'].strip()

        # 1) Primeira tentativa (mais "criativa")
        texto = chamar_groq(prompt, temperature=0.7, timeout=60)
        texto = limpar_markdown(texto)
        texto = corrigir_espacamento(texto)

        # limpeza p√≥s-processamento
        texto, removeu_fonte = remover_mencoes_de_fonte(texto)
        texto, removeu_titulo = remover_primeiro_paragrafo_se_repetir_titulo(texto, noticia.get('title', ''))

        flags = avaliar_qualidade_materia(noticia.get('title', ''), texto)
        if removeu_fonte:
            flags.append('pos_removeu_fonte')
        if removeu_titulo:
            flags.append('pos_removeu_titulo')

        # 2) Se a qualidade estiver ruim, tenta um segundo prompt mais r√≠gido
        if any(f in flags for f in ['nao_ptbr', 'menciona_fonte', 'repete_titulo', 'curto']):
            log(f"  ‚ö†Ô∏è Qualidade detectada ({', '.join(flags)}). Tentando reescrita mais r√≠gida...")
            texto2 = chamar_groq(prompt_strito, temperature=0.2, timeout=70)
            texto2 = limpar_markdown(texto2)
            texto2 = corrigir_espacamento(texto2)
            texto2, _ = remover_mencoes_de_fonte(texto2)
            texto2, _ = remover_primeiro_paragrafo_se_repetir_titulo(texto2, noticia.get('title', ''))
            flags2 = avaliar_qualidade_materia(noticia.get('title', ''), texto2)
            log(f"  üß™ Flags ap√≥s reescrita r√≠gida: {', '.join(flags2) if flags2 else 'ok'}")

            # escolhe o melhor texto (menos flags) e sempre rejeita se n√£o for PT-BR
            if 'nao_ptbr' in flags2:
                log("  üö´ Mat√©ria rejeitada: texto final ainda n√£o parece PT-BR")
                return None
            if len(flags2) <= len(flags):
                texto = texto2
                flags = flags2

        # √∫ltima valida√ß√£o: PT-BR obrigat√≥rio
        if not parece_portugues(texto):
            log("  üö´ Mat√©ria rejeitada: n√£o parece PT-BR")
            return None

        log(f"  ‚úÖ Mat√©ria gerada ({len(texto.split())} palavras) | flags: {', '.join(flags) if flags else 'ok'}")
        return texto
    except Exception as e:
        log(f"  ‚ö†Ô∏è Groq falhou: {str(e)[:60]}")
        log(f"  üìù Usando fallback (conte√∫do extra√≠do)...")
        return gerar_texto_fallback(noticia)

# Mapeamento global de subcategorias por categoria principal
SUBCATEGORIAS = {
    'esportes': {
        'futebol': ['futebol', 'flamengo', 'palmeiras', 'corinthians', 's√£o paulo', 'santos', 'vasco', 'botafogo', 'fluminense', 'gremio', 'internacional', 'cruzeiro', 'atletico', 'brasileir√£o', 'copa do brasil', 'libertadores', 'mundial', 'sele√ß√£o brasileira', 'neymar', 'messi', 'cristiano ronaldo', 'mbappe', 'haaland'],
        'automobilismo': ['f√≥rmula 1', 'formula 1', 'f1', 'stock car', 'nascar', 'rally', 'motogp', 'verstappen', 'hamilton', 'leclerc', 'p√©rez', 'alonso', 'sainz', 'norris', 'piastri', 'pilotos', 'gp', 'grande pr√™mio', 'corrida'],
        'basquete': ['nba', 'basquete', 'lebron', 'jordan', 'curry', 'durant', 'giannis', 'lakers', 'celtics', 'warriors', 'bulls', 'playoffs', 'finals'],
        'olimpiadas': ['olimp√≠adas', 'olimpiadas', 'paris 2024', 'los angeles 2028', 'atletismo', 'nata√ß√£o', 'gin√°stica', 'jud√¥', 'v√¥lei', 'handebol']
    },
    'entretenimento': {
        'cinema-series': ['filme', 'cinema', 's√©rie', 'netflix', 'hbo', 'disney+', 'amazon prime', 'star+', 'paramount', 'trailer', 'estreia', 'bilheteria', 'oscar', 'emmy', 'globo de ouro', 'ator', 'atriz', 'diretor', 'cinebiografia'],
        'musica': ['m√∫sica', 'banda', 'cantor', 'cantora', 'show', 'turn√™', '√°lbum', 'single', 'grammy', 'rock', 'pop', 'sertanejo', 'funk', 'rap', 'hip hop', 'anitta', 'taylor swift', 'beyonc√©', 'the weeknd', 'drake'],
        'cultura-pop': ['marvel', 'dc', 'star wars', 'harry potter', 'anime', 'mang√°', 'cosplay', 'conven√ß√£o', 'ccxp', 'comic con', 'super-her√≥i', 'vingadores', 'batman', 'superman', 'homem-aranha'],
        'teatro': ['teatro', 'pe√ßa', 'musical', 'broadway', 'west end', 'drama', 'com√©dia', 'atua√ß√£o', 'palco']
    },
    'tecnologia': {
        'hardware': ['hardware', 'processador', 'cpu', 'gpu', 'placa de v√≠deo', 'mem√≥ria ram', 'ssd', 'hd', 'notebook', 'desktop', 'pc', 'gamer', 'intel', 'amd', 'nvidia', 'cooler', 'fonte'],
        'software': ['software', 'windows', 'linux', 'macos', 'android', 'ios', 'aplicativo', 'app', 'programa', 'sistema operacional', 'atualiza√ß√£o', 'microsoft', 'google'],
        'inteligencia-artificial': ['intelig√™ncia artificial', 'ia', 'ai', 'chatgpt', 'gpt', 'llm', 'machine learning', 'deep learning', 'neural', 'openai', 'google gemini', 'claude', 'copilot', 'bard'],
        'ciberseguranca': ['ciberseguran√ßa', 'hacker', 'v√≠rus', 'malware', 'ransomware', 'phishing', 'golpe', 'fraude', 'vazamento de dados', 'privacidade', 'senha', 'autentica√ß√£o']
    },
    'videogames': {
        'noticias': ['jogo', 'novo jogo', 'lan√ßamento', 'trailer', 'gameplay', 'revelado', 'anunciado', 'confirmado', 'adiado', 'cancelado'],
        'reviews': ['review', 'an√°lise', 'nota', 'avalia√ß√£o', 'impress√µes', 'primeiras impress√µes', 'testamos', 'jogamos'],
        'esports': ['esports', 'e-sports', 'campeonato', 'torneio', 'competitivo', 'valorant', 'cs2', 'counter-strike', 'lol', 'league of legends', 'dota', 'fortnite', 'free fire', 'rainbow six'],
        'indies': ['indie', 'jogo independente', 'steam', 'itch.io', 'pixel art', 'roguelike', 'metroidvania', 'desenvolvedor independente']
    },
    'politica-nacional': {
        'congresso': ['c√¢mara', 'senado', 'congresso', 'deputado', 'senador', 'vota√ß√£o', 'projeto de lei', 'pec', 'impeachment', 'cpi', 'comiss√£o'],
        'governo-federal': ['lula', 'bolsonaro', 'presidente', 'ministro', 'governo', 'planalto', 'pt', 'pl', 'psdb', 'mdb', 'uni√£o brasil', 'executivo'],
        'eleicoes': ['elei√ß√£o', 'elei√ß√µes', 'campanha', 'candidato', 'pesquisa', 'ibope', 'datafolha', 'urna eletr√¥nica', 'voto', 'debate', 'hor√°rio eleitoral'],
        'justica': ['stf', 'supremo', 'alexandre de moraes', 'rosa weber', 'barroso', 'fachin', 'ministro do stf', 'pgr', 'pol√≠cia federal', 'lava jato', 'pris√£o', 'condena√ß√£o']
    },
    'politica-internacional': {
        'eua': ['eua', 'estados unidos', 'biden', 'trump', 'casa branca', 'pent√°gono', 'congresso americano', 'republicanos', 'democratas', 'elei√ß√µes americanas'],
        'europa': ['ue', 'uni√£o europeia', 'alemanha', 'fran√ßa', 'inglaterra', 'reino unido', 'italia', 'espanha', 'macron', 'scholz', 'sunak', 'meloni', 'brexit', 'nato', 'otan'],
        'asia': ['china', 'xi jinping', 'taiwan', 'jap√£o', '√≠ndia', 'coreia do norte', 'coreia do sul', 'putin', 'r√∫ssia', 'ucr√¢nia', 'guerra', 'tens√£o', 'brics'],
        'america-latina': ['argentina', 'chile', 'col√¥mbia', 'venezuela', 'nicar√°gua', 'cuba', 'mexico', 'milei', 'boric', 'maduro', 'ortega', 'l√≥pez obrador']
    },
    'rio-de-janeiro': {
        'seguranca': ['crime', 'pol√≠cia', 'pm', 'bope', 'tr√°fico', 'mil√≠cia', 'viol√™ncia', 'assalto', 'roubo', 'homic√≠dio', 'favela', 'complexo', 'tiroteio'],
        'transporte': ['√¥nibus', 'metr√¥', 'brt', 'trem', 'supervia', 'linha amarela', 'linha vermelha', 'ponte', 't√∫nel', 'engarrafamento', 'transito'],
        'cultura-eventos': ['carnaval', 'r√©veillon', 'rock in rio', 'show', 'festa', 'praia', 'copacabana', 'ipanema', 'cristo', 'p√£o de a√ß√∫car', 'museu', 'teatro municipal']
    },
    'sao-paulo': {
        'economia-negocios': ['bolsa', 'bovespa', 'empresas', 'startup', 'faria lima', 'paulista', 'itaim', 'vila ol√≠mpia', 'economia', 'neg√≥cios', 'investimentos'],
        'transporte': ['metro', 'metr√¥', 'cptm', '√¥nibus', 'marginal', 'paulista', 'congestionamento', 'rod√≠zio', 'bilhete √∫nico', 'linha amarela', 'linha verde'],
        'cultura-lazer': ['parque', 'ibirapuera', 'museu', 'masp', 'pinacoteca', 'teatro', 'show', 'evento', 'exposi√ß√£o', 'bienal', 'parada gay', 'virada cultural']
    }
}

def classificar_subcategoria_ia(titulo, categoria_principal):
    """Classifica subcategoria usando IA (Groq) quando palavras-chave n√£o funcionam"""
    if categoria_principal not in SUBCATEGORIAS:
        return None
    
    subcats_disponiveis = list(SUBCATEGORIAS[categoria_principal].keys())
    
    prompt = f"""Classifique o seguinte t√≠tulo de not√≠cia em UMA das subcategorias listadas.

T√≠tulo: "{titulo}"
Categoria principal: {categoria_principal}
Subcategorias dispon√≠veis: {', '.join(subcats_disponiveis)}

Responda APENAS com o nome exato da subcategoria mais adequada, sem explica√ß√£o. Se nenhuma se encaixar, responda "nenhuma"."""

    try:
        resp = requests.post(
            'https://api.groq.com/openai/v1/chat/completions',
            headers={'Authorization': f'Bearer {GROQ_API_KEY}', 'Content-Type': 'application/json'},
            json={
                'model': 'llama-3.3-70b-versatile',
                'messages': [{'role': 'user', 'content': prompt}],
                'temperature': 0.1,
                'max_tokens': 50
            },
            timeout=15
        )
        resp.raise_for_status()
        resultado = resp.json()['choices'][0]['message']['content'].strip().lower()
        
        # Valida se a resposta √© uma subcategoria v√°lida
        if resultado in subcats_disponiveis:
            log(f"  ü§ñ Subcategoria via IA: {resultado}")
            return resultado
        
        # Tenta match parcial (ex: "cinema e s√©ries" -> "cinema-series")
        for subcat in subcats_disponiveis:
            if subcat in resultado or resultado in subcat:
                log(f"  ü§ñ Subcategoria via IA (parcial): {subcat}")
                return subcat
        
        log(f"  ü§ñ IA n√£o classificou subcategoria: {resultado}")
        return None
    except Exception as e:
        log(f"  ‚ö†Ô∏è Classifica√ß√£o IA falhou: {str(e)[:40]}")
        return None

def classificar_subcategoria(titulo, categoria_principal):
    """Classifica automaticamente a subcategoria: primeiro por palavras-chave, depois por IA"""
    titulo_lower = titulo.lower()
    
    # Verifica se a categoria principal tem subcategorias definidas
    if categoria_principal not in SUBCATEGORIAS:
        return None
    
    # PASSO 1: Procura por palavras-chave no t√≠tulo (r√°pido e sem custo)
    cat_subs = SUBCATEGORIAS[categoria_principal]
    for subcat, palavras in cat_subs.items():
        if any(palavra in titulo_lower for palavra in palavras):
            log(f"  üè∑Ô∏è Subcategoria via keywords: {subcat}")
            return subcat
    
    # PASSO 2: Fallback para classifica√ß√£o via IA (Groq)
    log(f"  üîç Keywords n√£o encontraram subcategoria, tentando IA...")
    return classificar_subcategoria_ia(titulo, categoria_principal)

def salvar_post(titulo, texto, img, cat, data, post_id, subcategoria=None):
    slug = titulo.lower()[:50].replace(' ', '-').replace('?', '').replace('!', '').replace('/', '-')
    fname = f"post-{post_id:04d}-{slug}.html"
    
    # Formata par√°grafos com fun√ß√£o melhorada
    paragrafos = formatar_paragrafos(texto)

    
    # HTML com styling melhorado
    subcat_html = f'<span class="post-subcategoria">{subcategoria.replace("-"," ").title()}</span>' if subcategoria else ''
    
    html = f"""<!DOCTYPE html>
<html lang="pt-BR">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:title" content="{titulo}">
<meta property="og:image" content="{img}">
<meta property="og:type" content="article">
<title>{titulo} - Vivimundo</title>
<link rel="stylesheet" href="../style.css">
</head>
<body>
<header><div class="container"><h1 class="logo">VIVIMUNDO</h1>
<nav>
<a href="../index.html">In√≠cio</a>
<a href="../categoria-esportes.html">Esportes</a>
<a href="../categoria-entretenimento.html">Entretenimento</a>
<a href="../categoria-tecnologia.html">Tecnologia</a>
<a href="../categoria-videogames.html">Videogames</a>
<a href="../categoria-politica-nacional.html">Pol√≠tica Nacional</a>
<a href="../categoria-politica-internacional.html">Pol√≠tica Internacional</a>
<a href="../categoria-rio-de-janeiro.html">Rio de Janeiro</a>
<a href="../categoria-sao-paulo.html">S√£o Paulo</a>
<a href="../sobre.html">Sobre</a>
</nav>
</div></header>
<main class="container">
<article class="post-completo">
<div class="post-header">
<span class="post-categoria">{cat.replace('-',' ').title()}</span>
{subcat_html}
<h1 class="post-titulo">{titulo}</h1>
<div class="post-data">Por Kevin Ribeiro ‚Ä¢ {data}</div>
</div>
<img src="{img}" class="post-principal-imagem" alt="{titulo}" loading="lazy">
<div class="post-conteudo">
{paragrafos}
</div>
</article>
</main>
<footer><div class="container"><p>¬© 2026 Vivimundo</p><a href="https://x.com/Kevin_RSP0" target="_blank">Twitter</a></div></footer>
</body></html>"""
    
    Path("posts").mkdir(exist_ok=True)
    with open(Path("posts") / fname, 'w', encoding='utf-8') as f:
        f.write(html)
    log(f"  üíæ Post salvo: {fname}")
    return {'titulo': titulo, 'url': f"posts/{fname}", 'imagem': img, 'categoria': cat, 'subcategoria': subcategoria, 'data': data}


def atualizar_home(posts):
    cards = ""
    # Lista todas as mat√©rias (sem limite), em ordem decrescente (mais recentes primeiro)
    for p in reversed(posts):
        # Verifica se o arquivo HTML do post existe
        post_file = Path(p['url'])
        if not post_file.exists():
            log(f"  ‚ö†Ô∏è Post {p['titulo'][:40]} n√£o tem arquivo HTML, pulando")
            continue
        
        # Adiciona subcategoria se existir
        subcat_html = f'<span class="subcategoria">{p.get("subcategoria", "").replace("-"," ").title()}</span>' if p.get('subcategoria') else ''
        
        cards += f"""<article class="post-card">
<img src="{p['imagem']}" alt="{p['titulo']}">
<div class="post-info">
<span class="categoria categoria-{p['categoria']}">{p['categoria'].replace('-',' ').title()}</span>
{subcat_html}
<h2><a href="{p['url']}">{p['titulo']}</a></h2>
<p class="meta">Por Kevin Ribeiro ‚Ä¢ {p['data']}</p>
</div>
</article>"""
    
    html = f"""<!DOCTYPE html>
<html lang="pt-BR">
<head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Vivimundo - Portal de Not√≠cias</title><link rel="stylesheet" href="style.css"></head>
<body>
<header><div class="container"><h1 class="logo">VIVIMUNDO</h1>
<nav>
<a href="index.html">In√≠cio</a>
<a href="categoria-esportes.html">Esportes</a>
<a href="categoria-entretenimento.html">Entretenimento</a>
<a href="categoria-tecnologia.html">Tecnologia</a>
<a href="categoria-videogames.html">Videogames</a>
<a href="categoria-politica-nacional.html">Pol√≠tica Nacional</a>
<a href="categoria-politica-internacional.html">Pol√≠tica Internacional</a>
<a href="categoria-rio-de-janeiro.html">Rio de Janeiro</a>
<a href="categoria-sao-paulo.html">S√£o Paulo</a>
<a href="sobre.html">Sobre</a>
</nav>
</div></header>
<main class="container">
<h2 class="secao-titulo">√öltimas Not√≠cias</h2>
<div class="posts-grid">{cards}</div>
</main>
<footer><div class="container"><p>¬© 2026 Vivimundo</p><a href="https://x.com/Kevin_RSP0" target="_blank">Twitter</a></div></footer>
</body></html>"""
    with open("index.html", 'w', encoding='utf-8') as f:
        f.write(html)
    log("  üìù Index atualizado")


def gerar_paginas_categorias(posts):
    """Gera p√°ginas para cada categoria com artigos filtrados"""
    # Garante que todas as categorias do TEMAS tenham p√°ginas (mesmo que vazias)
    categorias = {tema['categoria']: [] for tema in TEMAS}
    
    # Preenche com posts existentes
    for p in posts:
        cat = p['categoria']
        if cat in categorias:
            categorias[cat].append(p)
    
    for cat, artigos in categorias.items():

        cards = ""
        # Lista todas as mat√©rias da categoria (sem limite), mais recentes primeiro
        for p in reversed(artigos):
            # Verifica se o arquivo HTML do post existe
            post_file = Path(p['url'])
            if not post_file.exists():
                continue
            
            # Adiciona subcategoria se existir
            subcat_html = f'<span class="subcategoria">{p.get("subcategoria", "").replace("-"," ").title()}</span>' if p.get('subcategoria') else ''
            
            cards += f"""<article class="post-card">
<img src="{p['imagem']}" alt="{p['titulo']}">
<div class="post-info">
<span class="categoria categoria-{p['categoria']}">{p['categoria'].replace('-',' ').title()}</span>
{subcat_html}
<h2><a href="{p['url']}">{p['titulo']}</a></h2>
<p class="meta">Por Kevin Ribeiro ‚Ä¢ {p['data']}</p>
</div>
</article>"""
        
        # Mensagem quando n√£o h√° artigos
        if not artigos:
            cards = '<p class="sem-artigos">Nenhuma not√≠cia nesta categoria ainda.</p>'
        
        html = f"""<!DOCTYPE html>
<html lang="pt-BR">
<head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>{cat.replace('-',' ').title()} - Vivimundo</title><link rel="stylesheet" href="style.css"></head>
<body>
<header><div class="container"><h1 class="logo">VIVIMUNDO</h1>
<nav>
<a href="index.html">In√≠cio</a>
<a href="categoria-esportes.html">Esportes</a>
<a href="categoria-entretenimento.html">Entretenimento</a>
<a href="categoria-tecnologia.html">Tecnologia</a>
<a href="categoria-videogames.html">Videogames</a>
<a href="categoria-politica-nacional.html">Pol√≠tica Nacional</a>
<a href="categoria-politica-internacional.html">Pol√≠tica Internacional</a>
<a href="categoria-rio-de-janeiro.html">Rio de Janeiro</a>
<a href="categoria-sao-paulo.html">S√£o Paulo</a>
<a href="sobre.html">Sobre</a>
</nav>
</div></header>
<main class="container">
<h2 class="secao-titulo">{cat.replace('-',' ').title()}</h2>
<div class="posts-grid">{cards}</div>
</main>
<footer><div class="container"><p>¬© 2026 Vivimundo</p><a href="https://x.com/Kevin_RSP0" target="_blank">Twitter</a></div></footer>
</body></html>"""

        
        fname = f"categoria-{cat}.html"
        with open(fname, 'w', encoding='utf-8') as f:
            f.write(html)
        log(f"  üìö Categoria '{cat}' atualizada")


def publicar():
    try:
        result = subprocess.run(['git', 'status', '--porcelain'], capture_output=True, text=True)
        if not result.stdout.strip():
            log("  ‚ö†Ô∏è Nada para commitar")
            return
        subprocess.run(['git', 'add', '.'], check=True)
        subprocess.run(['git', 'commit', '-m', f'Nova mat√©ria - {datetime.now().strftime("%d/%m/%Y %H:%M")}'], check=True)
        log("  ‚úÖ Commit realizado! (Push ser√° feito pelo GitHub Actions)")
    except Exception as e:
        log(f"  ‚ùå Commit: {e}")

def executar():
    pfile = Path("posts.json")
    posts = json.load(open(pfile)) if pfile.exists() else []
    tema_idx, total_posts = carregar_estado()
    tema = TEMAS[tema_idx]

    log(f"\n{'='*60}")
    log(f"üîÑ POST #{total_posts + 1} - {tema['nome']}")
    log(f"{'='*60}")
    
    noticia = buscar_noticia(tema)
    if not noticia:
        log("‚ùå Nenhuma not√≠cia encontrada")
        return
    
    texto = gerar_texto(noticia)
    if not texto:
        log("‚ö†Ô∏è Sem conte√∫do para salvar")
        return

    # Classifica subcategoria automaticamente
    subcategoria = classificar_subcategoria(noticia['title'], tema['categoria'])
    if subcategoria:
        log(f"  üè∑Ô∏è Subcategoria: {subcategoria}")
    
    info = salvar_post(noticia['title'], texto, noticia.get('urlToImage'), tema['categoria'], datetime.now().strftime('%d/%m/%Y √†s %H:%M'), total_posts + 1, subcategoria)

    posts.append(info)
    json.dump(posts, open(pfile, 'w'), ensure_ascii=False, indent=2)
    atualizar_home(posts)
    gerar_paginas_categorias(posts)
    publicar()

    # Salva estado para pr√≥xima execu√ß√£o
    tema_idx = (tema_idx + 1) % len(TEMAS)
    salvar_estado(tema_idx, total_posts + 1)
    
    # Evitar disparos excessivos em curto intervalo (prote√ß√£o contra loop infinito)
    # A cada 5 posts, espera 5 minutos antes do pr√≥ximo ciclo
    if (total_posts + 1) % 5 == 0:
        log(f"  ‚è≥ Pausa de prote√ß√£o: aguardando 5 minutos antes do pr√≥ximo ciclo...")
        time.sleep(300)
    
    log("\n‚úÖ CICLO CONCLU√çDO!")

if __name__ == "__main__":
    log("üåç VIVIMUNDO BOT - GitHub Actions")
    setup_repo()
    executar()
