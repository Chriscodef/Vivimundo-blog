#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import time
import json
import requests
from datetime import datetime
from pathlib import Path
import subprocess
import random
from bs4 import BeautifulSoup
import urllib3

# Desabilitar SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

sys.stdout.reconfigure(line_buffering=True, encoding='utf-8')
sys.stderr.reconfigure(line_buffering=True, encoding='utf-8')
os.environ['PYTHONIOENCODING'] = 'utf-8'

def log(msg):
    print(msg, flush=True)

# Configura√ß√µes
GROQ_API_KEY = os.getenv('GROQ_API_KEY')
GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')
REPO_PATH = os.getenv('GITHUB_WORKSPACE', '.')
if not GROQ_API_KEY:
    log("‚ùå GROQ_API_KEY n√£o encontrada!")
    sys.exit(1)

# Arquivo para salvar estado
STATE_FILE = Path(REPO_PATH) / "bot_state.json"
ARTICLES_CACHE = Path(REPO_PATH) / "articles_cache.json"

def carregar_cache_artigos():
    """Carrega URLs e t√≠tulos j√° processados"""
    if ARTICLES_CACHE.exists():
        with open(ARTICLES_CACHE, 'r') as f:
            data = json.load(f)
            if isinstance(data, dict):
                return set(data.get('urls', [])), set(data.get('titulos', []))
            # Compatibilidade com formato antigo (apenas URLs)
            return set(data), set()
    return set(), set()

def salvar_cache_artigos(urls, titulos):
    """Salva URLs e t√≠tulos processados"""
    with open(ARTICLES_CACHE, 'w') as f:
        json.dump({'urls': list(urls), 'titulos': list(titulos)}, f)

def normalizar_url(url):
    """Normaliza URL para compara√ß√£o consistente no cache"""
    from urllib.parse import urlparse, urlunparse, parse_qs, urlencode
    
    # Converte para lowercase
    url = url.lower().strip()
    
    # Remove trailing slash
    if url.endswith('/'):
        url = url[:-1]
    
    # Parse URL
    parsed = urlparse(url)
    
    # Remove par√¢metros de tracking comuns (utm_, fbclid, etc)
    query_params = parse_qs(parsed.query)
    params_limpos = {k: v for k, v in query_params.items() 
                     if not k.startswith(('utm_', 'fbclid', 'gclid', 'ref'))}
    
    # Reconstr√≥i query string ordenada
    nova_query = urlencode(params_limpos, doseq=True) if params_limpos else ''
    
    # Reconstr√≥i URL normalizada
    return urlunparse((
        parsed.scheme,
        parsed.netloc,
        parsed.path,
        parsed.params,
        nova_query,
        ''  # Remove fragmento (#)
    ))

def normalizar_titulo(titulo):
    """Normaliza t√≠tulo para detec√ß√£o de duplicatas"""
    import re
    # Remove espa√ßos extras, converte para lowercase
    titulo = titulo.lower().strip()
    # Remove pontua√ß√£o
    titulo = re.sub(r'[^\w\s]', '', titulo)
    # Remove espa√ßos m√∫ltiplos
    titulo = re.sub(r'\s+', ' ', titulo)
    return titulo


def carregar_estado():
    """Carrega o √≠ndice do √∫ltimo tema executado"""
    if STATE_FILE.exists():
        with open(STATE_FILE, 'r') as f:
            state = json.load(f)
            return state.get('tema_idx', 0), state.get('total_posts', 0)
    return 0, 0

def salvar_estado(tema_idx, total_posts):
    """Salva o √≠ndice do tema para pr√≥xima execu√ß√£o"""
    with open(STATE_FILE, 'w') as f:
        json.dump({'tema_idx': tema_idx, 'total_posts': total_posts}, f)

TEMAS = [
    {"nome": "Esportes", "categoria": "esportes", "sites": ["https://ge.globo.com/", "https://www.espn.com.br/", "https://www.uol.com.br/esporte/", "https://www.espn.com.br/futebol/", "https://www.grandepremio.com.br/"]},
    {"nome": "Entretenimento", "categoria": "entretenimento", "sites": ["https://www.omelete.com.br/", "https://www.tecmundo.com.br/cultura", "https://noticiasdocinema.com.br/"]},
    {"nome": "Tecnologia", "categoria": "tecnologia", "sites": ["https://www.tecmundo.com.br/", "https://olhardigital.com.br/", "https://www.hardware.com.br/", "https://www.tecmundo.com.br/voxel", "https://tecnoblog.net/"]},
    {"nome": "Videogames", "categoria": "videogames", "sites": ["https://www.gamerant.com/", "https://www.ign.com.br/", "https://www.thegamer.com.br/", "https://br.ign.com/"]},
    {"nome": "Pol√≠tica Nacional", "categoria": "politica-nacional", "sites": ["https://g1.globo.com/politica/", "https://noticias.uol.com.br/politica/", "https://www.folhapress.com.br/", "https://www.poder360.com.br/"]},
    {"nome": "Pol√≠tica Internacional", "categoria": "politica-internacional", "sites": ["https://g1.globo.com/mundo/", "https://www.bbc.com/portuguese/internacional", "https://noticias.uol.com.br/internacional/", "https://hojenomundomilitar.com.br/"]},
    {"nome": "Rio de Janeiro", "categoria": "rio-de-janeiro", "sites": ["https://g1.globo.com/rj/rio-de-janeiro/", "https://odia.ig.com.br/"]},
    {"nome": "S√£o Paulo", "categoria": "sao-paulo", "sites": ["https://g1.globo.com/sp/sao-paulo/"]}
]


HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
}

def setup_repo():
    try:
        log("üìÇ Configurando Git...")
        subprocess.run(['git', 'config', 'user.name', 'Vivimundo Bot'], check=True)
        subprocess.run(['git', 'config', 'user.email', 'bot@vivimundo.com'], check=True)
        if GITHUB_TOKEN:
            repo_url = f'https://{GITHUB_TOKEN}@github.com/Chriscodef/Vivimundo-blog.git'
            subprocess.run(['git', 'remote', 'remove', 'origin'], capture_output=True)
            subprocess.run(['git', 'remote', 'add', 'origin', repo_url], check=True, capture_output=True)
        subprocess.run(['git', 'pull', 'origin', 'main', '--rebase'], check=False)
        log("‚úÖ Git OK")
        return True
    except Exception as e:
        log(f"‚ö†Ô∏è {e}")
        return True

def extrair_imagem_meta(soup, url):
    """Extrai imagem de meta tags (og:image, twitter:image)"""
    try:
        # Tenta og:image primeiro
        img = soup.find('meta', property='og:image')
        if img and img.get('content'):
            return img['content']
        
        # Tenta twitter:image
        img = soup.find('meta', attrs={'name': 'twitter:image'})
        if img and img.get('content'):
            return img['content']
        
        # Tenta img com classe espec√≠fica
        img = soup.find('img', class_=lambda x: x and any(palavra in str(x).lower() for palavra in ['article', 'post', 'destaque', 'noticia', 'manchete']))
        if img and img.get('src'):
            return img['src']
    except:
        pass
    return None

def limpar_titulo(titulo):
    """Limpa t√≠tulos com palavras grudadas (ex: 'JacksonVeja' -> 'Jackson Veja')"""
    import re
    
    # Padr√£o 1: letra min√∫scula seguida de mai√∫scula (ex: "JacksonVeja")
    titulo = re.sub(r'([a-z])([A-Z])', r'\1 \2', titulo)
    
    # Padr√£o 2: pontua√ß√£o seguida de letra mai√∫scula sem espa√ßo (ex: "A√ç!Baldur's")
    titulo = re.sub(r'([!?:.])([A-Z])', r'\1 \2', titulo)
    
    # Padr√£o 3: palavra completamente mai√∫scula seguida de palavra capitalizada (ex: "HPComo")
    titulo = re.sub(r'([A-Z]{2,})([A-Z][a-z])', r'\1 \2', titulo)
    
    # Remove espa√ßos m√∫ltiplos
    titulo = re.sub(r'\s+', ' ', titulo)
    
    return titulo.strip()

def eh_titulo_valido(titulo):
    """Valida se o t√≠tulo √© real (n√£o √© n√∫mero de telefone, sequ√™ncia, etc)"""
    # Remove espa√ßos extras
    titulo = titulo.strip()
    
    # Muito curto ou longo
    if len(titulo) < 15 or len(titulo) > 250:
        return False
    
    # Parece n√∫mero de telefone ou ID
    if titulo.replace('-', '').replace('(', '').replace(')', '').isdigit():
        return False
    
    # Muitos n√∫meros (telefone, CEP, etc)
    num_count = sum(1 for c in titulo if c.isdigit())
    if num_count > len(titulo) * 0.3:  # Mais de 30% n√∫meros
        return False
    
    # Palavras v√°lidas m√≠nimas (n√£o √© s√≥ n√∫meros e s√≠mbolos)
    palavras = [p for p in titulo.split() if len(p) > 2 and not p.isdigit()]
    if len(palavras) < 3:  # Menos de 3 palavras v√°lidas
        return False
    
    # Rejeita t√≠tulos gen√©ricos de se√ß√£o (n√£o s√£o not√≠cias reais)
    titulo_lower = titulo.lower()
    palavras_secao = [
        'advance', 'latest', 'more', 'daily', 'special', 'featured',
        'esportes a motor', 'game rant', 'puzzles and games',
        'trending', 'popular', 'recommended', 'breaking'
    ]
    for palavra in palavras_secao:
        if palavra in titulo_lower:
            log(f"  üö´ T√≠tulo rejeitado (se√ß√£o gen√©rica): {titulo[:60]}...")
            return False
    
    # Rejeita t√≠tulos muito curtos com poucas palavras significativas (provavelmente categorias)
    palavras_significativas = [p for p in titulo.split() if len(p) > 3 and p.isalpha()]
    if len(palavras_significativas) < 4:
        # Verifica se parece uma categoria (sem verbos de a√ß√£o)
        verbos_acao = ['ganha', 'lan√ßa', 'confirma', 'aprova', 'revela', 'anuncia', 
                       'chega', 'vence', 'perde', 'encontra', 'descobre', 'morre',
                       'nasce', 'cresce', 'cai', 'sobe', 'muda', 'fica', 'vai', 'vem']
        tem_verbo = any(verbo in titulo_lower for verbo in verbos_acao)
        if not tem_verbo:
            log(f"  üö´ T√≠tulo rejeitado (sem verbo de a√ß√£o): {titulo[:60]}...")
            return False
    
    return True


def buscar_noticia(tema):
    time.sleep(random.uniform(1, 3))
    urls_processadas, titulos_processados = carregar_cache_artigos()
    
    for site_url in tema['sites']:

        try:
            log(f"  üîç Tentando {site_url}...")
            
            resp = requests.get(site_url, headers=HEADERS, timeout=20, verify=False)
            resp.encoding = 'utf-8'
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # Busca links em artigos, posts ou se√ß√µes de not√≠cias
            links = soup.find_all('a', href=True)
            links = links[:80]  # Aumentar para buscar mais links
            
            for link in links:
                href = link.get('href', '')
                titulo = link.get_text(strip=True)
                
                # Limpa t√≠tulos grudados
                titulo = limpar_titulo(titulo)
                
                # Valida t√≠tulo
                if not eh_titulo_valido(titulo):

                    continue
                
                # Palavras-chave para excluir
                palavras_bloqueadas = [
                    'publicidade', 'an√∫ncio', 'assine', 'login', 'cadastro', 'newsletter',
                    'amazon', 'aliexpress', 'mercado livre', 'shopee', 'custo', 'pre√ßo',
                    'compre', 'oferta', 'desconto', 'cupom', 'promo√ß√£o', 'black friday',
                    'aviso', 'clique', 'compartilhe', 'siga', 'inscreva', 'download',
                    'vpn', 'antiv√≠rus', 'norton', 'testegr√°tis', 'teste gr√°tis', '% off', '% offert',
                    'c√≥digo', 'cupom', 'deal', 'cyber', 'viagem', 'hotel', 'passagem',
                    'fone', 'fones', 'headphone', 'smartphone', 'iphone', 'samsung'
                ]
                
                if any(palavra in titulo.lower() for palavra in palavras_bloqueadas):
                    continue
                
                # Formata URL relativa
                if href.startswith('/'):
                    from urllib.parse import urljoin
                    href = urljoin(site_url, href)
                
                if not href.startswith('http'):
                    continue
                
                # Normaliza URL para verifica√ß√£o
                href_normalizada = normalizar_url(href)
                
                # Pula URL j√° processada (verifica√ß√£o normalizada)
                if href_normalizada in urls_processadas:
                    log(f"  üîÑ URL j√° processada: {href[:50]}...")
                    continue
                
                # Verifica duplicata por t√≠tulo normalizado
                titulo_normalizado = normalizar_titulo(titulo)
                if titulo_normalizado in titulos_processados:
                    log(f"  üîÑ T√≠tulo duplicado: {titulo[:50]}...")
                    continue

                
                # Bloqueia links para plataformas de compra
                urls_bloqueadas = ['amazon.com', 'aliexpress.com', 'mercadolivre.com', 'shopee.com', 'ebay.com']
                if any(bloqueado in href.lower() for bloqueado in urls_bloqueadas):
                    continue
                
                try:
                    time.sleep(random.uniform(0.7, 1.5))
                    
                    # Acessa artigo
                    art_resp = requests.get(href, headers=HEADERS, timeout=20, verify=False)
                    art_resp.encoding = 'utf-8'
                    art_soup = BeautifulSoup(art_resp.text, 'html.parser')
                    
                    # Remove lixo
                    for tag in art_soup(['script', 'style', 'nav', 'footer', 'aside']):
                        tag.decompose()
                    
                    # Busca conte√∫do em par√°grafos
                    paragrafos = art_soup.find_all('p')
                    texto = ' '.join(p.get_text(strip=True) for p in paragrafos if len(p.get_text(strip=True)) > 30)
                    
                    # Se n√£o encontrou em <p>, tenta em divs com classes de artigo
                    if len(texto) < 400:
                        article = art_soup.find(['article', 'div', 'main'], class_=lambda x: x and any(palavra in str(x).lower() for palavra in ['article', 'post', 'content', 'corpo', 'noticia', 'body', 'text']))
                        if article:
                            paragrafos = article.find_all('p')
                            texto = ' '.join(p.get_text(strip=True) for p in paragrafos if len(p.get_text(strip=True)) > 30)
                    
                    # Busca imagem com fun√ß√£o melhorada
                    img_url = extrair_imagem_melhorada(art_soup, href)
                    
                    # Formata URL da imagem
                    if img_url and not img_url.startswith('http'):
                        from urllib.parse import urljoin
                        img_url = urljoin(href, img_url)
                    
                    # Rejeita not√≠cias sem imagem real
                    if not img_url:
                        log(f"  üö´ Not√≠cia sem imagem, pulando: {titulo[:50]}...")
                        urls_processadas.add(href_normalizada)
                        titulos_processados.add(titulo_normalizado)
                        salvar_cache_artigos(urls_processadas, titulos_processados)
                        continue
                    
                    # Valida conte√∫do
                    if len(texto) > 500:
                        log(f"  ‚úÖ Encontrada: {titulo[:60]}...")
                        # Marca como processada (URL normalizada e t√≠tulo)
                        urls_processadas.add(href_normalizada)
                        titulos_processados.add(titulo_normalizado)
                        salvar_cache_artigos(urls_processadas, titulos_processados)
                        return {
                            'title': titulo, 
                            'content': texto, 
                            'urlToImage': img_url, 
                            'url': href
                        }

                    else:
                        # Marca como processada mesmo sem conte√∫do suficiente
                        urls_processadas.add(href_normalizada)
                        salvar_cache_artigos(urls_processadas, titulos_processados)


                except requests.exceptions.Timeout:
                    log(f"  ‚è± Timeout em {href[:40]}")
                    continue
                except Exception as e:
                    continue
            
            log(f"  ‚ö†Ô∏è Nada encontrado em {site_url}")
        except Exception as e:
            log(f"  ‚ùå Erro em {site_url}: {str(e)[:60]}")
            continue
    
    return None

def limpar_markdown(texto):
    """Remove formata√ß√£o markdown do texto"""
    import re
    # Remove **texto** -> texto
    texto = re.sub(r'\*\*(.*?)\*\*', r'\1', texto)
    # Remove *texto* -> texto
    texto = re.sub(r'\*(.*?)\*', r'\1', texto)
    # Remove __texto__ -> texto
    texto = re.sub(r'__(.*?)__', r'\1', texto)
    # Remove # titulo -> titulo
    texto = re.sub(r'^#+\s+', '', texto, flags=re.MULTILINE)
    # Remove tags HTML malformadas
    texto = re.sub(r'<p><h\d>(.*?)</h\d></p>', r'\1', texto)
    texto = re.sub(r'<p><p>(.*?)</p></p>', r'\1', texto)
    # Remove tags HTML abertas
    texto = re.sub(r'<h\d>|</h\d>', '', texto)
    return texto

def formatar_paragrafos(texto):
    """Formata texto em par√°grafos HTML bem estruturados"""
    import re
    # Limpa markdown primeiro
    texto = limpar_markdown(texto)
    
    # Remove tags HTML restantes
    texto = re.sub(r'<[^>]+>', '', texto)
    
    # Divide em par√°grafos por quebras duplas ou por pontos finais
    blocos = texto.split('\n\n')
    
    html = ""
    for bloco in blocos:
        bloco = bloco.strip()
        if len(bloco) > 50:  # Ignora blocos muito pequenos
            # Remove espa√ßos m√∫ltiplos
            bloco = re.sub(r'\s+', ' ', bloco)
            html += f'<p>{bloco}</p>\n'
    
    return html

def extrair_imagem_melhorada(soup, url):
    """Extrai a melhor imagem do artigo"""
    try:
        # Tenta og:image primeiro (mais confi√°vel)
        img = soup.find('meta', property='og:image')
        if img and img.get('content'):
            img_url = img['content']
            # Evita logos e √≠cones
            if not any(x in img_url.lower() for x in ['logo', 'icon', 'badge', 'avatar', 'profile']):
                return img_url
        
        # Tenta twitter:image
        img = soup.find('meta', attrs={'name': 'twitter:image'})
        if img and img.get('content'):
            return img['content']
        
        # Procura por imagem grande no artigo
        imgs = soup.find_all('img')
        melhor_img = None
        melhor_tamanho = 0
        
        for img in imgs:
            src = img.get('src', '')
            alt = img.get('alt', '')
            
            # Ignora logos, √≠cones, banners pequenos
            if any(x in src.lower() or x in alt.lower() for x in ['logo', 'icon', 'badge', 'avatar', 'gif', 'svg', 'button']):
                continue
            
            # Prefere imagens com atributos de tamanho
            width = img.get('width', '0')
            height = img.get('height', '0')
            try:
                tamanho = int(width) * int(height) if width and height else 0
                if tamanho > melhor_tamanho:
                    melhor_tamanho = tamanho
                    melhor_img = src
            except:
                if src and not melhor_img:
                    melhor_img = src
        
        return melhor_img
    except:
        pass
    return None

def gerar_texto_fallback(noticia):
    """Gera texto com fallback quando Groq falha"""
    titulo = noticia['title']
    conteudo = noticia.get('content', '')[:2000]
    
    # Estrutura b√°sica de mat√©ria
    paragrafos = conteudo.split('\n\n')
    texto = f"{titulo}\n\n"
    
    for i, p in enumerate(paragrafos[:10]):
        if len(p.strip()) > 50:
            texto += f"{p.strip()}\n\n"
    
    # Se ficou muito curto, repete o conte√∫do
    if len(texto) < 800:
        texto += "\n" + conteudo
    
    return texto[:3000]  # Limita a 3000 caracteres

def gerar_texto(noticia):
    prompt = f"""Escreva uma mat√©ria jornal√≠stica completa em portugu√™s brasileiro (m√≠nimo 450 palavras, par√°grafos, tom profissional) sobre:

T√≠tulo: {noticia['title']}
Conte√∫do: {noticia.get('content', '')[:3000]}

N√£o mencione fontes. Seja objetivo. Use apenas HTML simples (sem markdown)."""
    try:
        resp = requests.post(
            'https://api.groq.com/openai/v1/chat/completions',
            headers={'Authorization': f'Bearer {GROQ_API_KEY}', 'Content-Type': 'application/json'},
            json={'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': prompt}], 'temperature': 0.7, 'max_tokens': 2000},
            timeout=60
        )
        resp.raise_for_status()
        texto = resp.json()['choices'][0]['message']['content'].strip()
        # Limpa markdown do texto gerado
        texto = limpar_markdown(texto)
        log(f"  ‚úÖ Mat√©ria gerada ({len(texto.split())} palavras)")
        return texto
    except Exception as e:
        log(f"  ‚ö†Ô∏è Groq falhou: {str(e)[:60]}")
        log(f"  üìù Usando fallback (conte√∫do extra√≠do)...")
        return gerar_texto_fallback(noticia)

def classificar_subcategoria(titulo, categoria_principal):
    """Classifica automaticamente a subcategoria usando regras de palavras-chave"""
    titulo_lower = titulo.lower()
    
    # Mapeamento de subcategorias por categoria principal
    subcategorias = {
        'esportes': {
            'futebol': ['futebol', 'flamengo', 'palmeiras', 'corinthians', 's√£o paulo', 'santos', 'vasco', 'botafogo', 'fluminense', 'gremio', 'internacional', 'cruzeiro', 'atletico', 'brasileir√£o', 'copa do brasil', 'libertadores', 'mundial', 'sele√ß√£o brasileira', 'neymar', 'messi', 'cristiano ronaldo', 'mbappe', 'haaland'],
            'automobilismo': ['f√≥rmula 1', 'formula 1', 'f1', 'stock car', 'nascar', 'rally', 'motogp', 'verstappen', 'hamilton', 'leclerc', 'p√©rez', 'alonso', 'sainz', 'norris', 'piastri', 'pilotos', 'gp', 'grande pr√™mio', 'corrida'],
            'basquete': ['nba', 'basquete', 'lebron', 'jordan', 'curry', 'durant', 'giannis', 'lakers', 'celtics', 'warriors', 'bulls', 'playoffs', 'finals'],
            'olimpiadas': ['olimp√≠adas', 'olimpiadas', 'paris 2024', 'los angeles 2028', 'atletismo', 'nata√ß√£o', 'gin√°stica', 'jud√¥', 'v√¥lei', 'handebol']
        },
        'entretenimento': {
            'cinema-series': ['filme', 'cinema', 's√©rie', 'netflix', 'hbo', 'disney+', 'amazon prime', 'star+', 'paramount', 'trailer', 'estreia', 'bilheteria', 'oscar', 'emmy', 'globo de ouro', 'ator', 'atriz', 'diretor', 'cinebiografia'],
            'musica': ['m√∫sica', 'banda', 'cantor', 'cantora', 'show', 'turn√™', '√°lbum', 'single', 'grammy', 'rock', 'pop', 'sertanejo', 'funk', 'rap', 'hip hop', 'anitta', 'taylor swift', 'beyonc√©', 'the weeknd', 'drake'],
            'cultura-pop': ['marvel', 'dc', 'star wars', 'harry potter', 'anime', 'mang√°', 'cosplay', 'conven√ß√£o', 'ccxp', 'comic con', 'super-her√≥i', 'vingadores', 'batman', 'superman', 'homem-aranha'],
            'teatro': ['teatro', 'pe√ßa', 'musical', 'broadway', 'west end', 'drama', 'com√©dia', 'atua√ß√£o', 'palco']
        },
        'tecnologia': {
            'hardware': ['hardware', 'processador', 'cpu', 'gpu', 'placa de v√≠deo', 'mem√≥ria ram', 'ssd', 'hd', 'notebook', 'desktop', 'pc', 'gamer', 'intel', 'amd', 'nvidia', 'cooler', 'fonte'],
            'software': ['software', 'windows', 'linux', 'macos', 'android', 'ios', 'aplicativo', 'app', 'programa', 'sistema operacional', 'atualiza√ß√£o', 'microsoft', 'google'],
            'inteligencia-artificial': ['intelig√™ncia artificial', 'ia', 'ai', 'chatgpt', 'gpt', 'llm', 'machine learning', 'deep learning', 'neural', 'openai', 'google gemini', 'claude', 'copilot', 'bard'],
            'ciberseguranca': ['ciberseguran√ßa', 'hacker', 'v√≠rus', 'malware', 'ransomware', 'phishing', 'golpe', 'fraude', 'vazamento de dados', 'privacidade', 'senha', 'autentica√ß√£o']
        },
        'videogames': {
            'noticias': ['jogo', 'novo jogo', 'lan√ßamento', 'trailer', 'gameplay', 'revelado', 'anunciado', 'confirmado', 'adiado', 'cancelado'],
            'reviews': ['review', 'an√°lise', 'nota', 'avalia√ß√£o', 'impress√µes', 'primeiras impress√µes', 'testamos', 'jogamos'],
            'esports': ['esports', 'e-sports', 'campeonato', 'torneio', 'competitivo', 'valorant', 'cs2', 'counter-strike', 'lol', 'league of legends', 'dota', 'fortnite', 'free fire', 'rainbow six'],
            'indies': ['indie', 'jogo independente', 'steam', 'itch.io', 'pixel art', 'roguelike', 'metroidvania', 'desenvolvedor independente']
        },
        'politica-nacional': {
            'congresso': ['c√¢mara', 'senado', 'congresso', 'deputado', 'senador', 'vota√ß√£o', 'projeto de lei', 'pec', 'impeachment', 'cpi', 'comiss√£o'],
            'governo-federal': ['lula', 'bolsonaro', 'presidente', 'ministro', 'governo', 'planalto', 'pt', 'pl', 'psdb', 'mdb', 'uni√£o brasil', 'executivo'],
            'eleicoes': ['elei√ß√£o', 'elei√ß√µes', 'campanha', 'candidato', 'pesquisa', 'ibope', 'datafolha', 'urna eletr√¥nica', 'voto', 'debate', 'hor√°rio eleitoral'],
            'justica': ['stf', 'supremo', 'alexandre de moraes', 'rosa weber', 'barroso', 'fachin', 'ministro do stf', 'pgr', 'pol√≠cia federal', 'lava jato', 'pris√£o', 'condena√ß√£o']
        },
        'politica-internacional': {
            'eua': ['eua', 'estados unidos', 'biden', 'trump', 'casa branca', 'pent√°gono', 'congresso americano', 'republicanos', 'democratas', 'elei√ß√µes americanas'],
            'europa': ['ue', 'uni√£o europeia', 'alemanha', 'fran√ßa', 'inglaterra', 'reino unido', 'italia', 'espanha', 'macron', 'scholz', 'sunak', 'meloni', 'brexit', 'nato', 'otan'],
            'asia': ['china', 'xi jinping', 'taiwan', 'jap√£o', '√≠ndia', 'coreia do norte', 'coreia do sul', 'putin', 'r√∫ssia', 'ucr√¢nia', 'guerra', 'tens√£o', 'brics'],
            'america-latina': ['argentina', 'chile', 'col√¥mbia', 'venezuela', 'nicar√°gua', 'cuba', 'mexico', 'milei', 'boric', 'maduro', 'ortega', 'l√≥pez obrador']
        },
        'rio-de-janeiro': {
            'seguranca': ['crime', 'pol√≠cia', 'pm', 'bope', 'tr√°fico', 'mil√≠cia', 'viol√™ncia', 'assalto', 'roubo', 'homic√≠dio', 'favela', 'complexo', 'tiroteio'],
            'transporte': ['√¥nibus', 'metr√¥', 'brt', 'trem', 'supervia', 'linha amarela', 'linha vermelha', 'ponte', 't√∫nel', 'engarrafamento', 'transito'],
            'cultura-eventos': ['carnaval', 'r√©veillon', 'rock in rio', 'show', 'festa', 'praia', 'copacabana', 'ipanema', 'cristo', 'p√£o de a√ß√∫car', 'museu', 'teatro municipal']
        },
        'sao-paulo': {
            'economia-negocios': ['bolsa', 'bovespa', 'empresas', 'startup', 'faria lima', 'paulista', 'itaim', 'vila ol√≠mpia', 'economia', 'neg√≥cios', 'investimentos'],
            'transporte': ['metro', 'metr√¥', 'cptm', '√¥nibus', 'marginal', 'paulista', 'congestionamento', 'rod√≠zio', 'bilhete √∫nico', 'linha amarela', 'linha verde'],
            'cultura-lazer': ['parque', 'ibirapuera', 'museu', 'masp', 'pinacoteca', 'teatro', 'show', 'evento', 'exposi√ß√£o', 'bienal', 'parada gay', 'virada cultural']
        }
    }
    
    # Verifica se a categoria principal tem subcategorias definidas
    if categoria_principal not in subcategorias:
        return None
    
    # Procura por palavras-chave no t√≠tulo
    cat_subs = subcategorias[categoria_principal]
    for subcat, palavras in cat_subs.items():
        if any(palavra in titulo_lower for palavra in palavras):
            return subcat
    
    # Se n√£o encontrou, retorna None (sem subcategoria)
    return None

def salvar_post(titulo, texto, img, cat, data, post_id, subcategoria=None):
    slug = titulo.lower()[:50].replace(' ', '-').replace('?', '').replace('!', '').replace('/', '-')
    fname = f"post-{post_id:04d}-{slug}.html"
    
    # Formata par√°grafos com fun√ß√£o melhorada
    paragrafos = formatar_paragrafos(texto)

    
    # HTML com styling melhorado
    subcat_html = f'<span class="post-subcategoria">{subcategoria.replace("-"," ").title()}</span>' if subcategoria else ''
    
    html = f"""<!DOCTYPE html>
<html lang="pt-BR">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:title" content="{titulo}">
<meta property="og:image" content="{img}">
<meta property="og:type" content="article">
<title>{titulo} - Vivimundo</title>
<link rel="stylesheet" href="../style.css">
</head>
<body>
<header><div class="container"><h1 class="logo">VIVIMUNDO</h1>
<nav>
<a href="../index.html">In√≠cio</a>
<a href="../categoria-esportes.html">Esportes</a>
<a href="../categoria-entretenimento.html">Entretenimento</a>
<a href="../categoria-tecnologia.html">Tecnologia</a>
<a href="../categoria-videogames.html">Videogames</a>
<a href="../categoria-politica-nacional.html">Pol√≠tica Nacional</a>
<a href="../categoria-politica-internacional.html">Pol√≠tica Internacional</a>
<a href="../categoria-rio-de-janeiro.html">Rio de Janeiro</a>
<a href="../categoria-sao-paulo.html">S√£o Paulo</a>
<a href="../sobre.html">Sobre</a>
</nav>
</div></header>
<main class="container">
<article class="post-completo">
<div class="post-header">
<span class="post-categoria">{cat.replace('-',' ').title()}</span>
{subcat_html}
<h1 class="post-titulo">{titulo}</h1>
<div class="post-data">Por Kevin Ribeiro ‚Ä¢ {data}</div>
</div>
<img src="{img}" class="post-principal-imagem" alt="{titulo}" loading="lazy">
<div class="post-conteudo">
{paragrafos}
</div>
</article>
</main>
<footer><div class="container"><p>¬© 2026 Vivimundo</p><a href="https://x.com/Kevin_RSP0" target="_blank">Twitter</a></div></footer>
</body></html>"""
    
    Path("posts").mkdir(exist_ok=True)
    with open(Path("posts") / fname, 'w', encoding='utf-8') as f:
        f.write(html)
    log(f"  üíæ Post salvo: {fname}")
    return {'titulo': titulo, 'url': f"posts/{fname}", 'imagem': img, 'categoria': cat, 'subcategoria': subcategoria, 'data': data}


def atualizar_home(posts):
    cards = ""
    for p in reversed(posts[-10:]):
        # Verifica se o arquivo HTML do post existe
        post_file = Path(p['url'])
        if not post_file.exists():
            log(f"  ‚ö†Ô∏è Post {p['titulo'][:40]} n√£o tem arquivo HTML, pulando")
            continue
        
        # Adiciona subcategoria se existir
        subcat_html = f'<span class="subcategoria">{p.get("subcategoria", "").replace("-"," ").title()}</span>' if p.get('subcategoria') else ''
        
        cards += f"""<article class="post-card">
<img src="{p['imagem']}" alt="{p['titulo']}">
<div class="post-info">
<span class="categoria categoria-{p['categoria']}">{p['categoria'].replace('-',' ').title()}</span>
{subcat_html}
<h2><a href="{p['url']}">{p['titulo']}</a></h2>
<p class="meta">Por Kevin Ribeiro ‚Ä¢ {p['data']}</p>
</div>
</article>"""
    
    html = f"""<!DOCTYPE html>
<html lang="pt-BR">
<head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Vivimundo - Portal de Not√≠cias</title><link rel="stylesheet" href="style.css"></head>
<body>
<header><div class="container"><h1 class="logo">VIVIMUNDO</h1>
<nav>
<a href="index.html">In√≠cio</a>
<a href="categoria-esportes.html">Esportes</a>
<a href="categoria-entretenimento.html">Entretenimento</a>
<a href="categoria-tecnologia.html">Tecnologia</a>
<a href="categoria-videogames.html">Videogames</a>
<a href="categoria-politica-nacional.html">Pol√≠tica Nacional</a>
<a href="categoria-politica-internacional.html">Pol√≠tica Internacional</a>
<a href="categoria-rio-de-janeiro.html">Rio de Janeiro</a>
<a href="categoria-sao-paulo.html">S√£o Paulo</a>
<a href="sobre.html">Sobre</a>
</nav>
</div></header>
<main class="container">
<h2 class="secao-titulo">√öltimas Not√≠cias</h2>
<div class="posts-grid">{cards}</div>
</main>
<footer><div class="container"><p>¬© 2026 Vivimundo</p><a href="https://x.com/Kevin_RSP0" target="_blank">Twitter</a></div></footer>
</body></html>"""
    with open("index.html", 'w', encoding='utf-8') as f:
        f.write(html)
    log("  üìù Index atualizado")


def gerar_paginas_categorias(posts):
    """Gera p√°ginas para cada categoria com artigos filtrados"""
    # Garante que todas as categorias do TEMAS tenham p√°ginas (mesmo que vazias)
    categorias = {tema['categoria']: [] for tema in TEMAS}
    
    # Preenche com posts existentes
    for p in posts:
        cat = p['categoria']
        if cat in categorias:
            categorias[cat].append(p)
    
    for cat, artigos in categorias.items():

        cards = ""
        for p in reversed(artigos[-20:]):
            # Verifica se o arquivo HTML do post existe
            post_file = Path(p['url'])
            if not post_file.exists():
                continue
            
            # Adiciona subcategoria se existir
            subcat_html = f'<span class="subcategoria">{p.get("subcategoria", "").replace("-"," ").title()}</span>' if p.get('subcategoria') else ''
            
            cards += f"""<article class="post-card">
<img src="{p['imagem']}" alt="{p['titulo']}">
<div class="post-info">
<span class="categoria categoria-{p['categoria']}">{p['categoria'].replace('-',' ').title()}</span>
{subcat_html}
<h2><a href="{p['url']}">{p['titulo']}</a></h2>
<p class="meta">Por Kevin Ribeiro ‚Ä¢ {p['data']}</p>
</div>
</article>"""
        
        # Mensagem quando n√£o h√° artigos
        if not artigos:
            cards = '<p class="sem-artigos">Nenhuma not√≠cia nesta categoria ainda.</p>'
        
        html = f"""<!DOCTYPE html>
<html lang="pt-BR">
<head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>{cat.replace('-',' ').title()} - Vivimundo</title><link rel="stylesheet" href="style.css"></head>
<body>
<header><div class="container"><h1 class="logo">VIVIMUNDO</h1>
<nav>
<a href="index.html">In√≠cio</a>
<a href="categoria-esportes.html">Esportes</a>
<a href="categoria-entretenimento.html">Entretenimento</a>
<a href="categoria-tecnologia.html">Tecnologia</a>
<a href="categoria-videogames.html">Videogames</a>
<a href="categoria-politica-nacional.html">Pol√≠tica Nacional</a>
<a href="categoria-politica-internacional.html">Pol√≠tica Internacional</a>
<a href="categoria-rio-de-janeiro.html">Rio de Janeiro</a>
<a href="categoria-sao-paulo.html">S√£o Paulo</a>
<a href="sobre.html">Sobre</a>
</nav>
</div></header>
<main class="container">
<h2 class="secao-titulo">{cat.replace('-',' ').title()}</h2>
<div class="posts-grid">{cards}</div>
</main>
<footer><div class="container"><p>¬© 2026 Vivimundo</p><a href="https://x.com/Kevin_RSP0" target="_blank">Twitter</a></div></footer>
</body></html>"""

        
        fname = f"categoria-{cat}.html"
        with open(fname, 'w', encoding='utf-8') as f:
            f.write(html)
        log(f"  üìö Categoria '{cat}' atualizada")


def publicar():
    try:
        result = subprocess.run(['git', 'status', '--porcelain'], capture_output=True, text=True)
        if not result.stdout.strip():
            log("  ‚ö†Ô∏è Nada para commitar")
            return
        subprocess.run(['git', 'add', '.'], check=True)
        subprocess.run(['git', 'commit', '-m', f'Nova mat√©ria - {datetime.now().strftime("%d/%m/%Y %H:%M")}'], check=True)
        log("  ‚úÖ Commit realizado! (Push ser√° feito pelo GitHub Actions)")
    except Exception as e:
        log(f"  ‚ùå Commit: {e}")

def executar():
    pfile = Path("posts.json")
    posts = json.load(open(pfile)) if pfile.exists() else []
    tema_idx, total_posts = carregar_estado()
    tema = TEMAS[tema_idx]

    log(f"\n{'='*60}")
    log(f"üîÑ POST #{total_posts + 1} - {tema['nome']}")
    log(f"{'='*60}")
    
    noticia = buscar_noticia(tema)
    if not noticia:
        log("‚ùå Nenhuma not√≠cia encontrada")
        return
    
    texto = gerar_texto(noticia)
    if not texto:
        log("‚ö†Ô∏è Sem conte√∫do para salvar")
        return

    # Classifica subcategoria automaticamente
    subcategoria = classificar_subcategoria(noticia['title'], tema['categoria'])
    if subcategoria:
        log(f"  üè∑Ô∏è Subcategoria: {subcategoria}")
    
    info = salvar_post(noticia['title'], texto, noticia.get('urlToImage'), tema['categoria'], datetime.now().strftime('%d/%m/%Y √†s %H:%M'), total_posts + 1, subcategoria)

    posts.append(info)
    json.dump(posts, open(pfile, 'w'), ensure_ascii=False, indent=2)
    atualizar_home(posts)
    gerar_paginas_categorias(posts)
    publicar()

    # Salva estado para pr√≥xima execu√ß√£o
    tema_idx = (tema_idx + 1) % len(TEMAS)
    salvar_estado(tema_idx, total_posts + 1)
    log("\n‚úÖ CICLO CONCLU√çDO!")

if __name__ == "__main__":
    log("üåç VIVIMUNDO BOT - GitHub Actions")
    setup_repo()
    executar()
